{"input": ["dataset\\wmt17-tr-en\\corpus.tc.bpe.tr.shuf", "dataset\\wmt17-tr-en\\corpus.tc.bpe.en.shuf"], "output": "train", "model": "transformer", "vocab": ["dataset\\wmt17-tr-en\\vocab.tr.txt", "dataset\\wmt17-tr-en\\vocab.en.txt"], "pad": "<pad>", "bos": "<eos>", "eos": "<eos>", "unk": "<unk>", "batch_size": 8, "fixed_batch_size": false, "min_length": 1, "max_length": 256, "buffer_size": 10000, "initializer_gain": 1.0, "initializer": "uniform_unit_scaling", "scale_l1": 0.0, "scale_l2": 0.0, "initial_step": 0, "warmup_steps": 4000, "train_steps": 100000, "update_cycle": 2, "optimizer": "Adam", "adam_beta1": 0.9, "adam_beta2": 0.98, "adam_epsilon": 1e-09, "adadelta_rho": 0.95, "adadelta_epsilon": 1e-07, "pattern": "", "clipping": "global_norm", "clip_grad_norm": 0.0, "learning_rate": 0.0007, "initial_learning_rate": 0.0, "learning_rate_schedule": "linear_warmup_rsqrt_decay", "learning_rate_boundaries": [0], "learning_rate_values": [0.0], "device_list": [0], "keep_checkpoint_max": 20, "keep_top_checkpoint_max": 5, "save_summary": true, "save_checkpoint_secs": 0, "save_checkpoint_steps": 1000, "eval_steps": 2000, "eval_secs": 0, "top_beams": 1, "beam_size": 4, "decode_batch_size": 32, "decode_alpha": 0.6, "decode_ratio": 1.0, "decode_length": 50, "validation": "dataset\\wmt17-tr-en\\dev.tc.bpe.tr", "references": "dataset\\wmt17-tr-en\\dev.tc.en", "hidden_size": 512, "filter_size": 2048, "num_heads": 8, "num_encoder_layers": 6, "num_decoder_layers": 6, "attention_dropout": 0.0, "residual_dropout": 0.1, "relu_dropout": 0.0, "label_smoothing": 0.1, "normalization": "after", "shared_embedding_and_softmax_weights": false, "shared_source_target_embedding": false}